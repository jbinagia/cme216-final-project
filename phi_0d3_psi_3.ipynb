{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL experiment for Φ = 0.3, Ψ = 3.0\n",
    "For Φ = 0.3, Ψ = 3.0, the gyrotactic particle swims relatively slow and easily reoriented by vorticity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run main.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = True # whether or not we want to enable specific plot style configurations when creating plots for a report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_Φ = 0.3 # swimming number = v_s/u_0       \n",
    "my_Ψ = 3.0  # stability number = B w_0. B is the characteristic time a perturbed cell takes to return \n",
    "            # to orientation ka if w = 0. smaller means swimming more aligned with ka. \n",
    "my_alpha0 = 1.0 \n",
    "method = \"expSARSA\"\n",
    "eps_decay = True\n",
    "my_eps0 = 1.0\n",
    "my_omega = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb71f0f60f34a65be50c6c0172754da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Q, Σ, smart, naive, hist_R_tot_smart, hist_R_tot_naive, smart_stored_histories, naive_stored_histories, \\\n",
    "        state_action_counter, chosen_actions, avg_Q_hist, initial_coords, theta_history \\\n",
    "            = training(alpha0=my_alpha0, Φ=my_Φ, Ψ=my_Ψ, method=method, eps0=my_eps0, \\\n",
    "                       eps_decay=eps_decay, omega=my_omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory for saving figures files\n",
    "directory_name = \"phi_0d3_psi_3_expSARSA\"\n",
    "create_figure_dir(directory_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check performance after training this agent with this set of parameters\n",
    "plot_total_reward_vs_episode(hist_R_tot_smart, hist_R_tot_naive, N = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may have too much exploration present if we consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The state that was visited least had \" + str(np.min(state_action_counter)) + \" encounters, which is \" + \\\n",
    "     str(round(np.min(state_action_counter)/np.sum(state_action_counter)*100,2)) + \"% of the total states visited\")\n",
    "print(\"Pure exploration would have each pair visited \", np.round(np.sum(state_action_counter)/N_actions/N_states),\\\n",
    "      \" times, i.e. \", np.round(1/N_actions/N_states*100,2), \"% of the time\")\n",
    "# heatmap normalized by total number of encountersb\n",
    "ax = sns.heatmap(state_action_counter/np.sum(state_action_counter), linewidth=0.5, \\\n",
    "            xticklabels = [\"up\",\"down\",\"right\",\"left\"], yticklabels = product_states, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 2\n",
    "Let's restart with linear decay of $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, Σ, smart, naive, hist_R_tot_smart, hist_R_tot_naive, smart_stored_histories, naive_stored_histories, \\\n",
    "        state_action_counter, chosen_actions, avg_Q_hist, initial_coords, theta_history \\\n",
    "            = training(alpha0=my_alpha0, Φ=my_Φ, Ψ=my_Ψ, method=method, eps0=my_eps0, \\\n",
    "                       eps_decay=eps_decay, omega=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory for saving figures files\n",
    "directory_name = \"phi_0d3_psi_3_expSARSA_omega_1\"\n",
    "create_figure_dir(directory_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check performance after training this agent with this set of parameters\n",
    "plot_total_reward_vs_episode(hist_R_tot_smart, hist_R_tot_naive, N = 1000, paper=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_gain(hist_R_tot_smart, hist_R_tot_naive, N=1000, paper=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_select_trajectories(smart_stored_histories, naive_stored_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's overlay their relative performance on this last episode onto a streamline plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create underlying quiver plot\n",
    "plt.figure(figsize=(14,6))\n",
    "history_X = np.array(smart.history_X_total)\n",
    "x = np.linspace(np.min(history_X[1:,0]), np.max(history_X[1:,0]), 35)\n",
    "z = np.linspace(np.min(history_X[1:,1]), np.max(history_X[1:,1]), 20)\n",
    "X, Z = np.meshgrid(x, z)\n",
    "ux, uz, w = tgv(X, Z)\n",
    "plt.quiver(X, Z, ux, uz)\n",
    "\n",
    "# create scatter plot for policy\n",
    "cmap = plt.get_cmap('viridis', 4)\n",
    "naive_history_X = np.array(naive.history_X_total)\n",
    "plt.scatter(naive_history_X[1:,0], naive_history_X[1:,1],s=3,color='orange')\n",
    "ax = plt.scatter(history_X[1:,0], history_X[1:,1],s=3,c=chosen_actions[:], cmap=cmap, \\\n",
    "                 vmin = -.5, vmax = 3.5)\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(ticks=np.arange(0,4))\n",
    "cbar.ax.set_yticklabels(['Up', 'Down', 'Right','Left']) \n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$z$')\n",
    "plt.savefig(\"Figures/\" + directory_name + \"/final-ep-actions.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create underlying quiver plot\n",
    "plt.figure(figsize=(14,6))\n",
    "history_X = np.array(smart.history_X_total)\n",
    "x = np.linspace(np.min(history_X[1:,0]), np.max(history_X[1:,0]), 35)\n",
    "z = np.linspace(np.min(history_X[1:,1]), np.max(history_X[1:,1]), 20)\n",
    "X, Z = np.meshgrid(x, z)\n",
    "ux, uz, w = tgv(X, Z)\n",
    "plt.quiver(X, Z, ux, uz)\n",
    "\n",
    "# create scatter plot for policy\n",
    "ax = plt.scatter(history_X[1:,0], history_X[1:,1],s=3,c=theta_history[:]/np.pi*180)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(r'$\\theta$',rotation=180)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$z$')\n",
    "plt.savefig(\"Figures/\" + directory_name + \"/final-ep-theta.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make comparisons for a particular episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "ep, history_X = smart_stored_histories[i]\n",
    "\n",
    "# create underlying quiver plot\n",
    "plt.figure(figsize=(3.5, 2)) if paper else plt.figure(figsize=(14,6))\n",
    "x = np.linspace(0.75*np.min(history_X[1:,0]), np.max(history_X[1:,0]*1.1), 25)\n",
    "z = np.linspace(0.75*np.min(history_X[1:,1]), np.max(history_X[1:,1]*1.1), 25)\n",
    "Z, X = np.meshgrid(x, z)\n",
    "ux, uz, w = tgv(X, Z)\n",
    "plt.quiver(X, Z, ux, uz)\n",
    "\n",
    "# create scatter plot for policy\n",
    "cmap = plt.get_cmap('viridis', 4)\n",
    "_, naive_history_X = naive_stored_histories[i]\n",
    "plt.scatter(naive_history_X[1:,0], naive_history_X[1:,1],s=1,color='orange')\n",
    "ax = plt.scatter(history_X[1:,0], history_X[1:,1],s=3,c=chosen_actions[:], cmap=cmap, \\\n",
    "                 vmin = -.5, vmax = 3.5)\n",
    "cbar = plt.colorbar(ticks=np.arange(0,4))\n",
    "cbar.ax.set_yticklabels(['Up', 'Down', 'Right','Left']) \n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$z$')\n",
    "plt.savefig(\"Figures/\" + directory_name + \"/final-ep-actions.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learned policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the policy learned in this case, we can alculate the matrix Qnorm, which is Q but each of its rows is normalized so as to range from 0 to 1. In this way, we can visualize the best action (column) for each state (row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(3,2))\n",
    "Qnorm = copy.deepcopy(Q)\n",
    "max_per_row = np.max(Q,1)\n",
    "min_per_row = np.min(Q,1)\n",
    "for row in range(Q.shape[0]):\n",
    "    Qnorm[row,:] = (Qnorm[row,:]-min_per_row[row])/(max_per_row[row]-min_per_row[row])\n",
    "ax = sns.heatmap(Qnorm, linewidth=0.5, xticklabels = [\"up\",\"down\",\"right\",\"left\"], yticklabels = product_states, \\\n",
    "                cmap = 'inferno')\n",
    "plt.savefig(\"Figures/\" + directory_name + \"/final-policy.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can also visualize Q directly to a sense of the relative value of the state-action pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(Q, linewidth=0.5, xticklabels = [\"up\",\"down\",\"right\",\"left\"], yticklabels = product_states, \\\n",
    "                cmap = 'inferno')\n",
    "plt.savefig(\"Figures/\" + directory_name + \"/final-policy-unnorm.pdf\", format=\"pdf\",  bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the report, we'd like to visualize a case where the smart particle outperforms the naive. Let's iterate untilw e find such a case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0\n",
    "while delta < 1.0: # 1.0 is approximate difference in total return once we have converged \n",
    "    smart, naive, R_tot_smart, R_tot_naive, chosen_actions, history_theta = sample_trajectory(Φ=my_Φ, Ψ=my_Ψ, Q=Q, Ns=4000, D0=0, Dr=0)\n",
    "    delta = R_tot_smart - R_tot_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_X = np.array(smart.history_X_total)\n",
    "naive_history_X = np.array(naive.history_X_total)\n",
    "\n",
    "# create underlying quiver plot\n",
    "plt.figure(figsize=(3.5, 2)) if paper else plt.figure(figsize=(14,6))\n",
    "x = np.linspace(1.1*np.min(history_X[1:,0]), np.max(history_X[1:,0]*1.1), 25)\n",
    "z = np.linspace(0.9*np.min(history_X[1:,1]), np.max(history_X[1:,1]*1.1), 25)\n",
    "X, Z = np.meshgrid(x, z)\n",
    "ux, uz, w = tgv(X, Z)\n",
    "plt.quiver(X, Z, ux, uz)\n",
    "\n",
    "# create scatter plot for policy\n",
    "cmap = plt.get_cmap('viridis', 4)\n",
    "plt.scatter(naive_history_X[1:,0], naive_history_X[1:,1],s=1,color='orange')\n",
    "ax = plt.scatter(history_X[1:,0], history_X[1:,1],s=3,c=chosen_actions[:], cmap=cmap, \\\n",
    "                 vmin = -.5, vmax = 3.5)\n",
    "cbar = plt.colorbar(ticks=np.arange(0,4))\n",
    "cbar.ax.set_yticklabels(['Up', 'Down', 'Right','Left']) \n",
    "cbar.ax.set_title(r'$k_a$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$z$')\n",
    "plt.savefig(\"Figures/\" + directory_name + \"/final-ep-actions.png\", format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_X = np.array(smart.history_X_total)\n",
    "naive_history_X = np.array(naive.history_X_total)\n",
    "\n",
    "# create underlying quiver plot\n",
    "plt.figure(figsize=(3.5, 2)) if paper else plt.figure(figsize=(14,6))\n",
    "x = np.linspace(1.1*np.min(history_X[1:,0]), np.max(history_X[1:,0]*1.1), 25)\n",
    "z = np.linspace(0.9*np.min(history_X[1:,1]), np.max(history_X[1:,1]*1.1), 25)\n",
    "X, Z = np.meshgrid(x, z)\n",
    "ux, uz, w = tgv(X, Z)\n",
    "plt.quiver(X, Z, ux, uz)\n",
    "\n",
    "# create scatter plot for policy\n",
    "plt.scatter(naive_history_X[1:,0], naive_history_X[1:,1],s=1,color='orange')\n",
    "ax = plt.scatter(history_X[1:,0], history_X[1:,1],s=3,c=theta_history[:]/np.pi*180,cmap='twilight_shifted')\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_title(r'$\\theta$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$z$')\n",
    "plt.savefig(\"Figures/\" + directory_name + \"/final-ep-theta.png\", format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
