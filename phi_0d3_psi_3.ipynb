{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL experiment for Φ = 0.3, Ψ = 3.0\n",
    "For Φ = 0.3, Ψ = 3.0, the gyrotactic particle swims relatively slow and easily reoriented by vorticity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run main.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = True # whether or not we want to enable specific plot style configurations when creating plots for a report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_Φ = 0.3 # swimming number = v_s/u_0       \n",
    "my_Ψ = 3.0  # stability number = B w_0. B is the characteristic time a perturbed cell takes to return \n",
    "            # to orientation ka if w = 0. smaller means swimming more aligned with ka. \n",
    "my_alpha0 = 1.0 \n",
    "method = \"expSARSA\"\n",
    "eps_decay = True\n",
    "my_eps0 = 1.0\n",
    "my_omega = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b048821a14b415aaba518a0be16846b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-57908fb4541f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0mstate_action_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchosen_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_Q_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_history\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             = training(alpha0=my_alpha0, Φ=my_Φ, Ψ=my_Ψ, method=method, eps0=my_eps0, \\\n\u001b[0;32m----> 4\u001b[0;31m                        eps_decay=eps_decay, omega=my_omega)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-2f95c96087ba>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(alpha0, Φ, Ψ, Ns, Ne, gamma, eps0, D0, Dr, n_updates, RIC, method, lr_decay, omega, eps_decay, Qin)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;31m# given selected action, update the state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mnaive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_kinematics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mΦ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mΨ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0msmart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_kinematics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mΦ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mΨ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0msmart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# only need to update smart particle since naive has ka = [0, 1] always\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-08b67f2a2a6d>\u001b[0m in \u001b[0;36mupdate_kinematics\u001b[0;34m(self, Φ, Ψ, D0, Dr, int_method)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mint_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"euler\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# calculate new translational and rotational velocity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_velocity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mΦ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mΨ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-08b67f2a2a6d>\u001b[0m in \u001b[0;36mcalc_velocity\u001b[0;34m(self, Φ, Ψ)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mka_dot_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mka\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mka\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mΨ\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mka\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mka_dot_p\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpz\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mΨ\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mka\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mka_dot_p\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Q, Σ, smart, naive, hist_R_tot_smart, hist_R_tot_naive, smart_stored_histories, naive_stored_histories, \\\n",
    "        state_action_counter, chosen_actions, avg_Q_hist, initial_coords, theta_history \\\n",
    "            = training(alpha0=my_alpha0, Φ=my_Φ, Ψ=my_Ψ, method=method, eps0=my_eps0, \\\n",
    "                       eps_decay=eps_decay, omega=my_omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory for saving figures files\n",
    "directory_name = \"phi_0d3_psi_3_expSARSA\"\n",
    "create_figure_dir(directory_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check performance after training this agent with this set of parameters\n",
    "plot_total_reward_vs_episode(hist_R_tot_smart, hist_R_tot_naive, N = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may have too much exploration present if we consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The state that was visited least had \" + str(np.min(state_action_counter)) + \" encounters, which is \" + \\\n",
    "     str(round(np.min(state_action_counter)/np.sum(state_action_counter)*100,2)) + \"% of the total states visited\")\n",
    "print(\"Pure exploration would have each pair visited \", np.round(np.sum(state_action_counter)/N_actions/N_states),\\\n",
    "      \" times, i.e. \", np.round(1/N_actions/N_states*100,2), \"% of the time\")\n",
    "# heatmap normalized by total number of encountersb\n",
    "ax = sns.heatmap(state_action_counter/np.sum(state_action_counter), linewidth=0.5, \\\n",
    "            xticklabels = [\"up\",\"down\",\"right\",\"left\"], yticklabels = product_states, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 2\n",
    "Let's restart with linear decay of $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, Σ, smart, naive, hist_R_tot_smart, hist_R_tot_naive, smart_stored_histories, naive_stored_histories, \\\n",
    "        state_action_counter, chosen_actions, avg_Q_hist, initial_coords, theta_history \\\n",
    "            = training(alpha0=my_alpha0, Φ=my_Φ, Ψ=my_Ψ, method=method, eps0=my_eps0, \\\n",
    "                       eps_decay=eps_decay, omega=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory for saving figures files\n",
    "directory_name = \"phi_0d3_psi_3_expSARSA_omega_1\"\n",
    "create_figure_dir(directory_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check performance after training this agent with this set of parameters\n",
    "plot_total_reward_vs_episode(hist_R_tot_smart, hist_R_tot_naive, N = 1000, paper=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_gain(hist_R_tot_smart, hist_R_tot_naive, N=1000, paper=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_select_trajectories(smart_stored_histories, naive_stored_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's overlay their relative performance on this last episode onto a streamline plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create underlying quiver plot\n",
    "plt.figure(figsize=(14,6))\n",
    "history_X = np.array(smart.history_X_total)\n",
    "x = np.linspace(np.min(history_X[1:,0]), np.max(history_X[1:,0]), 35)\n",
    "z = np.linspace(np.min(history_X[1:,1]), np.max(history_X[1:,1]), 20)\n",
    "X, Z = np.meshgrid(x, z)\n",
    "ux, uz, w = tgv(X, Z)\n",
    "plt.quiver(X, Z, ux, uz)\n",
    "\n",
    "# create scatter plot for policy\n",
    "cmap = plt.get_cmap('viridis', 4)\n",
    "naive_history_X = np.array(naive.history_X_total)\n",
    "plt.scatter(naive_history_X[1:,0], naive_history_X[1:,1],s=3,color='orange')\n",
    "ax = plt.scatter(history_X[1:,0], history_X[1:,1],s=3,c=chosen_actions[:], cmap=cmap, \\\n",
    "                 vmin = -.5, vmax = 3.5)\n",
    "\n",
    "\n",
    "cbar = plt.colorbar(ticks=np.arange(0,4))\n",
    "cbar.ax.set_yticklabels(['Up', 'Down', 'Right','Left']) \n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$z$')\n",
    "plt.savefig(\"Figures/\" + directory_name + \"/final-ep-actions.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create underlying quiver plot\n",
    "plt.figure(figsize=(14,6))\n",
    "history_X = np.array(smart.history_X_total)\n",
    "x = np.linspace(np.min(history_X[1:,0]), np.max(history_X[1:,0]), 35)\n",
    "z = np.linspace(np.min(history_X[1:,1]), np.max(history_X[1:,1]), 20)\n",
    "X, Z = np.meshgrid(x, z)\n",
    "ux, uz, w = tgv(X, Z)\n",
    "plt.quiver(X, Z, ux, uz)\n",
    "\n",
    "# create scatter plot for policy\n",
    "ax = plt.scatter(history_X[1:,0], history_X[1:,1],s=3,c=theta_history[:]/np.pi*180)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(r'$\\theta$',rotation=180)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$z$')\n",
    "plt.savefig(\"Figures/\" + directory_name + \"/final-ep-theta.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make comparisons for a particular episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "ep, history_X = smart_stored_histories[i]\n",
    "\n",
    "# create underlying quiver plot\n",
    "plt.figure(figsize=(3.5, 2)) if paper else plt.figure(figsize=(14,6))\n",
    "x = np.linspace(0.75*np.min(history_X[1:,0]), np.max(history_X[1:,0]*1.1), 25)\n",
    "z = np.linspace(0.75*np.min(history_X[1:,1]), np.max(history_X[1:,1]*1.1), 25)\n",
    "Z, X = np.meshgrid(x, z)\n",
    "ux, uz, w = tgv(X, Z)\n",
    "plt.quiver(X, Z, ux, uz)\n",
    "\n",
    "# create scatter plot for policy\n",
    "cmap = plt.get_cmap('viridis', 4)\n",
    "_, naive_history_X = naive_stored_histories[i]\n",
    "plt.scatter(naive_history_X[1:,0], naive_history_X[1:,1],s=1,color='orange')\n",
    "ax = plt.scatter(history_X[1:,0], history_X[1:,1],s=3,c=chosen_actions[:], cmap=cmap, \\\n",
    "                 vmin = -.5, vmax = 3.5)\n",
    "cbar = plt.colorbar(ticks=np.arange(0,4))\n",
    "cbar.ax.set_yticklabels(['Up', 'Down', 'Right','Left']) \n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$z$')\n",
    "plt.savefig(\"Figures/\" + directory_name + \"/final-ep-actions.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learned policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the policy learned in this case, we can alculate the matrix Qnorm, which is Q but each of its rows is normalized so as to range from 0 to 1. In this way, we can visualize the best action (column) for each state (row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(3,2))\n",
    "Qnorm = copy.deepcopy(Q)\n",
    "max_per_row = np.max(Q,1)\n",
    "min_per_row = np.min(Q,1)\n",
    "for row in range(Q.shape[0]):\n",
    "    Qnorm[row,:] = (Qnorm[row,:]-min_per_row[row])/(max_per_row[row]-min_per_row[row])\n",
    "ax = sns.heatmap(Qnorm, linewidth=0.5, xticklabels = [\"up\",\"down\",\"right\",\"left\"], yticklabels = product_states, \\\n",
    "                cmap = 'inferno')\n",
    "plt.savefig(\"Figures/\" + directory_name + \"/final-policy.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can also visualize Q directly to a sense of the relative value of the state-action pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(Q, linewidth=0.5, xticklabels = [\"up\",\"down\",\"right\",\"left\"], yticklabels = product_states, \\\n",
    "                cmap = 'inferno')\n",
    "plt.savefig(\"Figures/\" + directory_name + \"/final-policy-unnorm.pdf\", format=\"pdf\",  bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the report, we'd like to visualize a case where the smart particle outperforms the naive. Let's iterate untilw e find such a case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0\n",
    "while delta < 1.0: # 1.0 is approximate difference in total return once we have converged \n",
    "    smart, naive, R_tot_smart, R_tot_naive, chosen_actions, history_theta = sample_trajectory(Φ=my_Φ, Ψ=my_Ψ, Q=Q, Ns=4000, D0=0, Dr=0)\n",
    "    delta = R_tot_smart - R_tot_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_X = np.array(smart.history_X_total)\n",
    "naive_history_X = np.array(naive.history_X_total)\n",
    "\n",
    "# create underlying quiver plot\n",
    "plt.figure(figsize=(3.5, 2)) if paper else plt.figure(figsize=(14,6))\n",
    "x = np.linspace(1.1*np.min(history_X[1:,0]), np.max(history_X[1:,0]*1.1), 25)\n",
    "z = np.linspace(0.9*np.min(history_X[1:,1]), np.max(history_X[1:,1]*1.1), 25)\n",
    "X, Z = np.meshgrid(x, z)\n",
    "ux, uz, w = tgv(X, Z)\n",
    "plt.quiver(X, Z, ux, uz)\n",
    "\n",
    "# create scatter plot for policy\n",
    "cmap = plt.get_cmap('viridis', 4)\n",
    "plt.scatter(naive_history_X[1:,0], naive_history_X[1:,1],s=1,color='orange')\n",
    "ax = plt.scatter(history_X[1:,0], history_X[1:,1],s=3,c=chosen_actions[:], cmap=cmap, \\\n",
    "                 vmin = -.5, vmax = 3.5)\n",
    "cbar = plt.colorbar(ticks=np.arange(0,4))\n",
    "cbar.ax.set_yticklabels(['Up', 'Down', 'Right','Left']) \n",
    "cbar.ax.set_title(r'$k_a$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$z$')\n",
    "plt.savefig(\"Figures/\" + directory_name + \"/final-ep-actions.png\", format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_X = np.array(smart.history_X_total)\n",
    "naive_history_X = np.array(naive.history_X_total)\n",
    "\n",
    "# create underlying quiver plot\n",
    "plt.figure(figsize=(3.5, 2)) if paper else plt.figure(figsize=(14,6))\n",
    "x = np.linspace(1.1*np.min(history_X[1:,0]), np.max(history_X[1:,0]*1.1), 25)\n",
    "z = np.linspace(0.9*np.min(history_X[1:,1]), np.max(history_X[1:,1]*1.1), 25)\n",
    "X, Z = np.meshgrid(x, z)\n",
    "ux, uz, w = tgv(X, Z)\n",
    "plt.quiver(X, Z, ux, uz)\n",
    "\n",
    "# create scatter plot for policy\n",
    "plt.scatter(naive_history_X[1:,0], naive_history_X[1:,1],s=1,color='orange')\n",
    "ax = plt.scatter(history_X[1:,0], history_X[1:,1],s=3,c=theta_history[:]/np.pi*180,cmap='twilight_shifted')\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_title(r'$\\theta$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$z$')\n",
    "plt.savefig(\"Figures/\" + directory_name + \"/final-ep-theta.png\", format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
