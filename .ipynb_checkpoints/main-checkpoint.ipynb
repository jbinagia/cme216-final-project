{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "In this notebook, we will setup the necessary data structures and functions that will be constantly used during our set of reinforcement learning experiments.\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import plotly.graph_objects as go\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from wand.image import Image as WImage\n",
    "# sns.set(palette=\"husl\",font_scale=1)\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import copy\n",
    "np.random.seed(0)\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 2*np.pi # periodic domain size \n",
    "\n",
    "# define boundaries of simulation box\n",
    "x0 = 0      \n",
    "x1 = L\n",
    "z0 = 0\n",
    "z1 = L \n",
    "\n",
    "# define reinforcement learning problem \n",
    "N_states = 12 # number of states - one for each coarse-grained degree of vorticity \n",
    "N_actions = 4 # number of actions - one for each coarse-grained swimming direction\n",
    "\n",
    "# numerical parameters\n",
    "dt = 0.01 # timestep size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "# Runga-Kutta 4(5) integration for one step \n",
    "    # see https://stackoverflow.com/questions/54494770/how-to-set-fixed-step-size-with-scipy-integrate\n",
    "def DoPri45Step(f,t,x,h):\n",
    "\n",
    "    k1 = f(t,x)\n",
    "    k2 = f(t + 1./5*h, x + h*(1./5*k1) )\n",
    "    k3 = f(t + 3./10*h, x + h*(3./40*k1 + 9./40*k2) )\n",
    "    k4 = f(t + 4./5*h, x + h*(44./45*k1 - 56./15*k2 + 32./9*k3) )\n",
    "    k5 = f(t + 8./9*h, x + h*(19372./6561*k1 - 25360./2187*k2 + 64448./6561*k3 - 212./729*k4) )\n",
    "    k6 = f(t + h, x + h*(9017./3168*k1 - 355./33*k2 + 46732./5247*k3 + 49./176*k4 - 5103./18656*k5) )\n",
    "\n",
    "    v5 = 35./384*k1 + 500./1113*k3 + 125./192*k4 - 2187./6784*k5 + 11./84*k6\n",
    "    k7 = f(t + h, x + h*v5)\n",
    "    v4 = 5179./57600*k1 + 7571./16695*k3 + 393./640*k4 - 92097./339200*k5 + 187./2100*k6 + 1./40*k7;\n",
    "\n",
    "    return v4,v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define useful data structures\n",
    "### Define a dictionary of the possible states and their assigned indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_states = [\"right\",\"down\",\"left\",\"up\"] # coarse-grained directions\n",
    "vort_states = [\"w+\", \"w0\", \"w-\"] # coarse-grained levels of vorticity \n",
    "product_states = [(x,y) for x in direction_states for y in vort_states]  # all possible states\n",
    "state_lookup_table = {product_states[i]:i for i in range(len(product_states))} # returns index of given state\n",
    "# print(product_states) # to view mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an agent class for reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, Ns):\n",
    "        self.r = np.zeros(Ns) # reward for each stage\n",
    "        self.t = 0            # time\n",
    "        \n",
    "    # calculate reward given from entering a new state after a selected action is undertaken\n",
    "    def calc_reward(self):\n",
    "        # enforce implementation by subclass\n",
    "        if self.__class__ == AbstractClass:\n",
    "                raise NotImplementedError\n",
    "                \n",
    "    def update_state(self):\n",
    "        # enforce implementation by subclass\n",
    "        if self.__class__ == AbstractClass:\n",
    "                raise NotImplementedError\n",
    "                \n",
    "    def take_random_action(self):\n",
    "        # enforce implementation by subclass\n",
    "        if self.__class__ == AbstractClass:\n",
    "                raise NotImplementedError\n",
    "                \n",
    "    def take_greedy_action(self, Q):\n",
    "        # enforce implementation by subclass\n",
    "        if self.__class__ == AbstractClass:\n",
    "                raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define swimmer class derived from agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swimmer(Agent):\n",
    "    def __init__(self, Ns):\n",
    "        # call init for superclass\n",
    "        super().__init__(Ns)\n",
    "        \n",
    "        # local position within the periodic box. X = [x, z]^T with 0 <= x < 2 pi and 0 <= z < 2 pi\n",
    "        self.X = np.array([np.random.uniform(0, L), np.random.uniform(0, L)])\n",
    "        \n",
    "        # absolute position. -inf. <= x_total < inf. and -inf. <= z_total < inf.\n",
    "        self.X_total = self.X\n",
    "        \n",
    "        # particle orientation \n",
    "        self.theta = np.random.uniform(0, 2*np.pi) # polar angle theta in the x-z plane \n",
    "        self.p = np.array([np.cos(self.theta), np.sin(self.theta)]) # p = [px, pz]^T\n",
    "        \n",
    "        # translational and rotational velocity\n",
    "        self.U = np.zeros(2)\n",
    "        self.W = np.zeros(2)\n",
    "        \n",
    "        # preferred swimming direction (equal to [1,0], [0,1], [-1,0], or [0,-1])\n",
    "        self.ka = np.array([0,1])\n",
    "        \n",
    "        # history of local and global position. Only store information for this episode. \n",
    "        self.history_X = [self.X]\n",
    "        self.history_X_total = [self.X_total]\n",
    "        \n",
    "        # local vorticity at the current location\n",
    "        _, _, self.w = tgv(self.X[0], self.X[1])\n",
    "        \n",
    "        # update coarse-grained state\n",
    "        self.update_state()\n",
    "        \n",
    "    def reinitialize(self):\n",
    "        self.X = np.array([np.random.uniform(0, L), np.random.uniform(0, L)])\n",
    "        self.X_total = self.X\n",
    "        \n",
    "        self.theta = np.random.uniform(0, 2*np.pi) # polar angle theta in the x-z plane \n",
    "        self.p = np.array([np.cos(self.theta), np.sin(self.theta)]) # p = [px, pz]^T\n",
    "        \n",
    "        self.U = np.zeros(2)\n",
    "        self.W = np.zeros(2)\n",
    "\n",
    "        self.ka = np.array([0,1])\n",
    "\n",
    "        self.history_X = [self.X]\n",
    "        self.history_X_total = [self.X_total]\n",
    "        \n",
    "        self.t = 0 \n",
    "        \n",
    "    def update_kinematics(self, Φ, Ψ, D0 = 0, Dr = 0, int_method = \"euler\"):\n",
    "        if int_method == \"rk45\":\n",
    "            y0 = np.concatenate((self.X,self.p))\n",
    "            _, v5 = DoPri45Step(self.calc_velocity_rk45,self.t,y0,dt)\n",
    "            y = y0 + dt*v5\n",
    "            self.X = y[:2]\n",
    "            self.p = y[2:]\n",
    "            dx = self.X - self.history_X[-1]\n",
    "            self.X_total = self.X_total + dx \n",
    "            \n",
    "            # check if still in the periodic box\n",
    "            self.check_in_box()\n",
    "            \n",
    "            # ensure the vector p has unit length \n",
    "            self.p /= (self.p[0]**2 + self.p[1]**2)**(1/2)\n",
    "\n",
    "            # update polar angle\n",
    "            x = self.p[0]\n",
    "            yy = self.p[1]\n",
    "            self.theta = np.arctan2(yy,x) if yy >= 0 else (np.arctan2(yy,x) + 2*np.pi)\n",
    "\n",
    "            # store positions\n",
    "            self.history_X.append(self.X)\n",
    "            self.history_X_total.append(self.X_total)\n",
    "            \n",
    "        elif int_method == \"euler\":\n",
    "            # calculate new translational and rotational velocity \n",
    "            self.calc_velocity(Φ, Ψ)\n",
    "\n",
    "            self.update_position(int_method, D0)\n",
    "            self.update_orientation(int_method, Dr)\n",
    "        else:\n",
    "            raise Exception(\"Integration method must be 'Euler' or 'rk45'\")\n",
    "\n",
    "        self.t = self.t + dt\n",
    "    \n",
    "    def calc_velocity_rk45(self, t, y):\n",
    "        x = y[0]\n",
    "        z = y[1]\n",
    "        px = y[2]\n",
    "        pz = y[3]\n",
    "        ux, uz, self.w = tgv(x, z)\n",
    "        \n",
    "        U0 = ux + Φ*px\n",
    "        U1 = uz + Φ*pz\n",
    "        \n",
    "        ka_dot_p = self.ka[0]*px + self.ka[1]*pz\n",
    "        W0 = 1/2/Ψ*(self.ka[0] - ka_dot_p*px) + 1/2*pz*self.w\n",
    "        W1 = 1/2/Ψ*(self.ka[1] - ka_dot_p*pz) + 1/2*-px*self.w\n",
    "        \n",
    "        return np.array([U0, U1, W0, W1])\n",
    "        \n",
    "    \n",
    "    def update_position(self, int_method, D0):\n",
    "        # use explicit euler to update \n",
    "        dx = dt*self.U\n",
    "        if D0 > 0: dx = dx + np.sqrt(2*D0*dt)*np.random.normal(size=2)\n",
    "        self.X = self.X + dx\n",
    "        self.X_total = self.X_total + dx\n",
    "        \n",
    "        # check if still in the periodic box\n",
    "        self.check_in_box()\n",
    "        \n",
    "        # store positions\n",
    "        self.history_X.append(self.X)\n",
    "        self.history_X_total.append(self.X_total)\n",
    "        \n",
    "    \n",
    "    def update_orientation(self, int_method, Dr):\n",
    "        self.p = self.p + dt*self.W \n",
    "        \n",
    "        # ensure the vector p has unit length \n",
    "        self.p /= (self.p[0]**2 + self.p[1]**2)**(1/2)         \n",
    "        \n",
    "        # if rotational diffusion is present\n",
    "        if Dr > 0:\n",
    "            px = self.p[0]\n",
    "            pz = self.p[1]\n",
    "            cross = px*pz\n",
    "            A = np.array([[1-px**2, -cross], [-cross, 1-pz**2]])\n",
    "            v = np.sqrt(2*Dr*dt)*np.random.normal(size=2)\n",
    "            self.p[0] = self.p[0] + A[0,0]*v[0] + A[0,1]*v[1]\n",
    "            self.p[1] = self.p[1] + A[1,0]*v[0] + A[1,1]*v[1]\n",
    "            self.p /= (self.p[0]**2 + self.p[1]**2)**(1/2)\n",
    "\n",
    "        # update polar angle\n",
    "        x = self.p[0]\n",
    "        y = self.p[1]\n",
    "        self.theta = np.arctan2(y,x) if y >= 0 else (np.arctan2(y,x) + 2*np.pi)\n",
    "        \n",
    "\n",
    "    def calc_velocity(self, Φ, Ψ):\n",
    "        ux, uz, self.w = tgv(self.X[0], self.X[1])\n",
    "        \n",
    "        # careful - computing in the following way is significantly slower: self.U = np.array(ux, uz) + Φ*self.p\n",
    "        self.U[0] = ux + Φ*self.p[0]\n",
    "        self.U[1] = uz + Φ*self.p[1]\n",
    "        \n",
    "        px = self.p[0]\n",
    "        pz = self.p[1]\n",
    "        ka_dot_p = self.ka[0]*px + self.ka[1]*pz\n",
    "        self.W[0] = 1/2/Ψ*(self.ka[0] - ka_dot_p*px) + 1/2*pz*self.w \n",
    "        self.W[1] = 1/2/Ψ*(self.ka[1] - ka_dot_p*pz) + 1/2*-px*self.w \n",
    "        \n",
    "        \n",
    "    def check_in_box(self): \n",
    "        if self.X[0] < x0:\n",
    "            self.X[0] += L \n",
    "        elif self.X[0] > x1:\n",
    "            self.X[0] -= L \n",
    "        if self.X[1] < z0:\n",
    "            self.X[1] += L \n",
    "        elif self.X[1] > z1:\n",
    "            self.X[1] -= L    \n",
    "            \n",
    "    def calc_reward(self, n):\n",
    "        self.r[n] = self.history_X_total[-1][1]-self.history_X_total[-2][1]\n",
    "        \n",
    "    def update_state(self):\n",
    "        if self.w < -0.33:\n",
    "            w_state = \"w-\"\n",
    "        elif self.w >= -0.33 and self.w <= 0.33:\n",
    "            w_state = \"w0\"\n",
    "        elif self.w > 0.33:\n",
    "            w_state = \"w+\"\n",
    "        else:\n",
    "            raise Exception(\"Invalid value of w detected: \", w)\n",
    "\n",
    "        if self.theta >= np.pi/4 and self.theta < 3*np.pi/4:\n",
    "            p_state = \"up\"\n",
    "        elif self.theta >= 3*np.pi/4 and self.theta < 5*np.pi/4:\n",
    "            p_state = \"left\"\n",
    "        elif self.theta >= 5*np.pi/4 and self.theta < 7*np.pi/4:\n",
    "            p_state = \"down\"\n",
    "        elif (self.theta >= 7*np.pi/4 and self.theta <= 2*np.pi) or (self.theta >= 0 and self.theta < np.pi/4):\n",
    "            p_state = \"right\"\n",
    "        else:\n",
    "            raise Exception(\"Invalid value of theta detected: \", theta)\n",
    "\n",
    "        self.my_state = (p_state, w_state)\n",
    "        \n",
    "    def take_greedy_action(self, Q):\n",
    "        state_index = state_lookup_table[self.my_state]\n",
    "        action_index = np.argmax(Q[state_index])  # find largest entry in this row of Q (i.e. this state)\n",
    "        if action_index == 0:   # up\n",
    "            self.ka = [0, 1]\n",
    "        elif action_index == 1: # down\n",
    "            self.ka = [0, -1]\n",
    "        elif action_index == 2: # right\n",
    "            self.ka = [1, 0]\n",
    "        else:                   # left\n",
    "            self.ka = [-1, 0]\n",
    "        return action_index\n",
    "            \n",
    "    def take_random_action(self):\n",
    "        action_index = np.random.randint(4)\n",
    "        if action_index == 0:   # up\n",
    "            self.ka = [0, 1]\n",
    "        elif action_index == 1: # down\n",
    "            self.ka = [0, -1]\n",
    "        elif action_index == 2: # right\n",
    "            self.ka = [1, 0]\n",
    "        else:                   # left\n",
    "            self.ka = [-1, 0]\n",
    "        return action_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Taylor-Green vortex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given position, return local velocity and vorticity \n",
    "def tgv(x, z):\n",
    "    ux = -1/2*np.cos(x)*np.sin(z)\n",
    "    uz = 1/2*np.sin(x)*np.cos(z)\n",
    "    w = -np.cos(x)*np.cos(z)\n",
    "    return ux, uz, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the flow field\n",
    "\n",
    "# x = np.linspace(0,L,100)\n",
    "# z = np.linspace(0,L,100)\n",
    "# xv, zv = np.meshgrid(x, z)\n",
    "# ux, uz, w = tgv(xv, zv)\n",
    "\n",
    "# fig = go.Figure(data = go.Contour(x = x, y = z, z=w))\n",
    "\n",
    "# fig.update_layout(\n",
    "#     title=r\"$\\text{Vorticity }(w)$\",\n",
    "#     xaxis_title=\"$x$\",\n",
    "#     yaxis_title=\"$z$\"\n",
    "# )\n",
    "\n",
    "# fig.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the above make sense? consider $x=0$, $y=0$. There, the vorticity is large and negative. This makes sense in light of the streamplot shown below (considering the right-hand rule and noting that positive $y$ would point into the page since the coordinate-system is right-handed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.streamplot(x,z,ux,uz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function shown below defines our reinforcement learning procedure, using either Q-learning or double Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def training(alpha0, Φ, Ψ, Ns=4000, Ne=5000, gamma=0.999, eps0=0.0, D0=0, Dr=0, n_updates=1000, \\\n",
    "             RIC=False, method=\"Qlearning\", lr_decay=None, omega=0.85, eps_decay=False, Qin=None):\n",
    "    # n_updates - how often to plot the trajectory undertaken by the particle during the learning process\n",
    "    # Ne - number of episodes\n",
    "    # Ns - number of steps in an episode\n",
    "    # alpha0 - learning rate (or starting learning rate when employing LR decay)\n",
    "    # gamma - discount factor, i.e. how much we weigh future to present rewards. Close to 0 = myopic view. \n",
    "    # eps0 - fraction of the time we allow for exploration in selecting the following action. 0 = always greedy. \n",
    "    # D0 - translational diffusivity\n",
    "    # Dr - rotational diffusivity \n",
    "    # RIC - Reset of Initial Conditions. First time a state-action pair is encountered, set Q[s,a] = reward\n",
    "    # method - choose from Q-learning, Double Q-learning (, or Expected SARSA\n",
    "    # lr_decay - whether or not to use learning rate decay. Options are none, or polynomial (lr=1/#(s,a)**omega)\n",
    "    # omega - exponent used in lr_decay: lr = 1/#(s,a)**omega\n",
    "    # eps_decay - whether or not to decay epsilon linearly: eff_eps = eps0/k for the k-th step\n",
    "    # Qin - initial Q matrix. Useful for testing performance after an extensive exploration phase. \n",
    "\n",
    "    # if using the expected SARSA method, turn on epsilon decay since eps = 0 is simply Q-learning anyway\n",
    "    if method==\"expSARSA\": \n",
    "        eps_decay = True\n",
    "        if eps0 == 0: eps0 = 1\n",
    "    \n",
    "    # Total reward for each episode\n",
    "    hist_R_tot_smart = np.zeros(Ne)\n",
    "    hist_R_tot_naive= np.zeros(Ne)   \n",
    "\n",
    "    # learning gain per episode\n",
    "    Σ = np.zeros(Ne)            \n",
    "    \n",
    "    smart_stored_histories = []       # store position = f(t) every so often for an episode (smart particles)\n",
    "    naive_stored_histories = []       # store position = f(t) every so often for an episode (naive particles)\n",
    "    \n",
    "    # number of times each state-action pair has been explored\n",
    "    state_action_counter = np.zeros((N_states,N_actions))\n",
    "    \n",
    "    # initialize a naive and a smart gyrotactic particle\n",
    "    naive = Swimmer(Ns)\n",
    "    smart = Swimmer(Ns)\n",
    "    \n",
    "    # initialize Q matrix to large value \n",
    "    if method==\"doubleQ\":\n",
    "        Q1 = L*Ns*np.ones((12, 4))\n",
    "        Q2 = L*Ns*np.ones((12, 4))\n",
    "    else:\n",
    "        Q = L*Ns*np.ones((12, 4))   # 12 states, 4 possible actions. Each column is an action, ka.\n",
    "        \n",
    "    if Qin is not None: Q = Qin\n",
    "        \n",
    "    # store average Q for each episode to track convergence\n",
    "    avg_Q_history = np.zeros((Ne,12,4))\n",
    "    \n",
    "    # store initial position and orientation for each episode\n",
    "    initial_coords = np.zeros((Ne,3))\n",
    "\n",
    "    # iterate over episodes\n",
    "    k = 0\n",
    "    for ep in tqdm(range(Ne)):  \n",
    "\n",
    "        # assign random orientation and position \n",
    "        smart.reinitialize()\n",
    "        naive.reinitialize()\n",
    "        naive = copy.deepcopy(smart) # have naive and smart share initial conditions for visualization purposes\n",
    "        \n",
    "        # store initialization\n",
    "        initial_coords[ep,0:2] = smart.X\n",
    "        initial_coords[ep,2] = smart.theta\n",
    "        \n",
    "        # save selected actions and particle orientation for last episodes\n",
    "        if ep == Ne - 1: \n",
    "            chosen_actions = np.zeros(Ns)\n",
    "            theta_history = np.zeros(Ns)\n",
    "\n",
    "        # iterate over stages within an episode\n",
    "        for stage in range(Ns): \n",
    "\n",
    "            # select an action eps-greedily. Note naive never changes its action/strategy (i.e. trying to swim up)\n",
    "            Qinput = Q1 + Q2 if method==\"doubleQ\" else Q\n",
    "            k = k + 1 # k-th update \n",
    "            \n",
    "            eff_eps = eps0/k**omega if eps_decay else eps0 # decrease amount of exploration as time proceeds\n",
    "            if np.random.uniform(0, 1) < eff_eps:\n",
    "                action = smart.take_random_action()\n",
    "            else:\n",
    "                action = smart.take_greedy_action(Qinput)\n",
    "                \n",
    "            # record action and orientation on last episode\n",
    "            if ep == Ne - 1: \n",
    "                chosen_actions[stage] = action\n",
    "                theta_history[stage] = smart.theta\n",
    "\n",
    "            # record index of the prior state\n",
    "            old_s = state_lookup_table[smart.my_state]\n",
    "\n",
    "            # given selected action, update the state\n",
    "            naive.update_kinematics(Φ, Ψ, D0, Dr)\n",
    "            smart.update_kinematics(Φ, Ψ, D0, Dr)\n",
    "            smart.update_state()      # only need to update smart particle since naive has ka = [0, 1] always\n",
    "\n",
    "            # calculate reward based on new state\n",
    "            naive.calc_reward(stage)\n",
    "            smart.calc_reward(stage)\n",
    "\n",
    "            new_s = state_lookup_table[smart.my_state]\n",
    "            state_action_counter[new_s,action] += 1\n",
    "            \n",
    "            # employ learning rate decay if applicable \n",
    "            alpha = alpha0/(1+state_action_counter[old_s,action])**omega if lr_decay else alpha0\n",
    "            \n",
    "            # update Q matrix \n",
    "            if method==\"doubleQ\":\n",
    "                if np.random.uniform(0, 1) < 0.5: # update Q1\n",
    "                    if Q1[old_s, action] == L*Ns and RIC==True: # apply Reset of Initial Conditions (RIC)\n",
    "                        Q1[old_s, action] = smart.r[stage]\n",
    "                    else:\n",
    "                        Q1[old_s, action] = Q1[old_s, action] + alpha*(smart.r[stage] + \\\n",
    "                                gamma*np.max(Q2[new_s,:])-Q1[old_s,action])\n",
    "                else: # update Q2\n",
    "                    if Q2[old_s, action] == L*Ns and RIC==True: \n",
    "                        Q2[old_s, action] = smart.r[stage]\n",
    "                    else:\n",
    "                        Q2[old_s, action] = Q2[old_s, action] + alpha*(smart.r[stage] + \\\n",
    "                                gamma*np.max(Q1[new_s,:])-Q2[old_s,action])\n",
    "            if method==\"expSARSA\":\n",
    "                # calculate V, the expected Q value for the next state-actio pair\n",
    "                V = 0\n",
    "                greedy_action = np.argmax(Q[new_s]) # would-be greedy action for new state\n",
    "                for new_action in range(N_actions):\n",
    "                    pi = (1 - eff_eps) + eff_eps/N_actions if new_action == greedy_action else eff_eps/N_actions\n",
    "                    V = V + pi*Q[new_s, new_action]\n",
    "                    \n",
    "                if Q[old_s, action] == L*Ns and RIC==True: \n",
    "                    Q[old_s, action] = smart.r[stage]\n",
    "                else:\n",
    "                    Q[old_s, action] = Q[old_s, action] + alpha*(smart.r[stage] + gamma*V - Q[old_s,action])\n",
    "            else:\n",
    "                if Q[old_s, action] == L*Ns and RIC==True: \n",
    "                    Q[old_s, action] = smart.r[stage]\n",
    "                else:\n",
    "                    Q[old_s, action] = Q[old_s, action] + alpha*(smart.r[stage] + \\\n",
    "                            gamma*np.max(Q[new_s,:])-Q[old_s,action])\n",
    "                    \n",
    "            # store average Q for each episode to track convergence\n",
    "            avg_Q_history[ep] = avg_Q_history[ep] + Q1 + Q2 if method==\"doubleQ\" else avg_Q_history[ep] + Q\n",
    "            \n",
    "        avg_Q_history[ep] = avg_Q_history[ep]/Ns\n",
    " \n",
    "                \n",
    "        # calculate Rtot for this episode\n",
    "        R_tot_naive = np.sum(naive.r)\n",
    "        R_tot_smart = np.sum(smart.r)\n",
    "\n",
    "        # calculate learning gain for this episode\n",
    "        Σ[ep] = R_tot_smart/R_tot_naive - 1\n",
    "        hist_R_tot_smart[ep] = R_tot_smart\n",
    "        hist_R_tot_naive[ep] = R_tot_naive\n",
    "\n",
    "        # plot trajectory every so often \n",
    "        if ep%n_updates==0 or ep==Ne-1:\n",
    "            smart_history_X_total = np.array(smart.history_X_total)\n",
    "            smart_stored_histories.append((ep,smart_history_X_total))\n",
    "            naive_history_X_total = np.array(naive.history_X_total)\n",
    "            naive_stored_histories.append((ep,naive_history_X_total))\n",
    "        \n",
    "        # save optimal policy\n",
    "        if ep==Ne-1:\n",
    "            filename = \"Policies/Q_alpha_\" + str(alpha).replace(\".\",\"d\") + \"_Ns_\" + str(Ns) + \"_Ne_\" + str(Ne) + \\\n",
    "                    \"_Φ_\" + str(Φ).replace(\".\",\"d\") + \"_Ψ_\" + str(Ψ).replace(\".\",\"d\") + \"_eps_\" \\\n",
    "                    + str(eff_eps).replace(\".\",\"d\") + \"_epsdecay_\" + str(eps_decay)\n",
    "            if lr_decay: filename = filename + \"_omega_\" + str(omega)\n",
    "            if method==\"doubleQ\": filename = filename + \"_\" + str(method)\n",
    "            if RIC: filename = filename + \"_RIC_\" + str(RIC) \n",
    "            Qout = Q1 + Q2 if method==\"doubleQ\" else Q\n",
    "            np.save(filename, Qout)\n",
    "    \n",
    "    return Qout, Σ, smart, naive, hist_R_tot_smart, hist_R_tot_naive, smart_stored_histories, naive_stored_histories, \\\n",
    "        state_action_counter, chosen_actions, avg_Q_history, initial_coords, theta_history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the following function for the case where we'd like to sample a trajectory for a swimmer given an input Q. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectory(Φ, Ψ, Q, Ns=4000, D0=0, Dr=0): \n",
    "\n",
    "    # initialize a naive and a smart gyrotactic particle\n",
    "    smart = Swimmer(Ns)\n",
    "    naive = Swimmer(Ns)\n",
    "    naive = copy.deepcopy(smart) # have naive and smart share initial conditions for visualization purposes\n",
    "\n",
    "    # save selected actions and particle orientation \n",
    "    chosen_actions = np.zeros(Ns)\n",
    "    theta_history = np.zeros(Ns)\n",
    "\n",
    "    # iterate over stages within an episode\n",
    "    for stage in range(Ns): \n",
    "\n",
    "        # always select greedy action since we aren't exploring here\n",
    "        action = smart.take_greedy_action(Q)\n",
    "\n",
    "        # record action and orientation on last episode\n",
    "        chosen_actions[stage] = action\n",
    "        theta_history[stage] = smart.theta\n",
    "\n",
    "        # given selected action, update the state\n",
    "        naive.update_kinematics(Φ, Ψ, D0, Dr)\n",
    "        smart.update_kinematics(Φ, Ψ, D0, Dr)\n",
    "        smart.update_state()      # need to update so we know what actions to take\n",
    "        \n",
    "        # calculate reward based on new state\n",
    "        naive.calc_reward(stage)\n",
    "        smart.calc_reward(stage)\n",
    "            \n",
    "        # calculate Rtot for this episode\n",
    "        R_tot_naive = np.sum(naive.r)\n",
    "        R_tot_smart = np.sum(smart.r)\n",
    "    \n",
    "    return smart, naive, R_tot_smart, R_tot_naive, chosen_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing\n",
    "### Create new directory to store figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_figure_dir(directory_name):\n",
    "    if os.path.exists('Figures/' + directory_name):\n",
    "        ans = input(\"Warning: this folder already exists. Overwrite its contents? (y/n)\\n\")\n",
    "        if ans != \"y\": raise Exception(\"Stopping to prevent overwriting previous figures\")\n",
    "    else:\n",
    "        os.makedirs('Figures/' + directory_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the total reward as a function of the episode number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_total_reward_vs_episode(hist_R_tot_smart, hist_R_tot_naive, N=500):\n",
    "    # hist_R_tot_smart, hist_R_tot_naive - total reward for each episode for the smart and naive particles\n",
    "    # N - how many episodes to conduct the moving average over\n",
    "    \n",
    "    Ne = len(hist_R_tot_naive)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=np.arange(N,Ne+1), y=moving_average(hist_R_tot_smart,N), mode='lines', name = \"Smart\"))\n",
    "    fig.add_trace(go.Scatter(x=np.arange(N,Ne+1), y=moving_average(hist_R_tot_naive,N), mode='lines', name = \"Naive\"))\n",
    "    fig.update_layout(\n",
    "        title=\"Total reward vs. episode #\",\n",
    "        xaxis_title=\"Episode #\",\n",
    "        yaxis_title=\"Total reward, Rtot\",\n",
    "    )\n",
    "    fig.show()\n",
    "    fig.write_image(\"Figures/\" + directory_name + \"/total-reward.pdf\") # save figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning gain as a function of episode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_gain(hist_R_tot_smart, hist_R_tot_naive, N=500):\n",
    "    # hist_R_tot_smart, hist_R_tot_naive - total reward for each episode for the smart and naive particles\n",
    "    # N - how many episodes to conduct the moving average over\n",
    "    \n",
    "    Ne = len(hist_R_tot_naive)\n",
    "    fig = go.Figure()\n",
    "    Σ = moving_average(hist_R_tot_smart,N)/moving_average(hist_R_tot_naive,N)-1\n",
    "    fig.add_trace(go.Scatter(x=np.arange(N,Ne+1), y=Σ*100, mode='lines')) # multiply by 100 to get %\n",
    "    fig.update_layout(\n",
    "        title=\"Learning gain over time\",\n",
    "        xaxis_title=\"Episode, E\",\n",
    "        yaxis_title=r\"$\\text{Learning gain, }\\Sigma$\",\n",
    "    )\n",
    "    fig.show()\n",
    "    fig.write_image(\"Figures/\" + directory_name + \"/learning-gain.pdf\") # save figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot trajectories of smart and naive particle for selected episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_select_trajectories(smart_stored_histories, naive_stored_histories, style='matplotlib'):\n",
    "    for i in range(len(smart_stored_histories)):\n",
    "            ep, smart_history_X_total = smart_stored_histories[i]\n",
    "            ep, naive_history_X_total = naive_stored_histories[i]\n",
    "            Ns = smart_history_X_total.shape[0]-1\n",
    "\n",
    "            if i == len(smart_stored_histories) - 1: style = \"plotly\"\n",
    "            if style == \"plotly\":\n",
    "                fig = go.Figure(go.Scatter(x=smart_history_X_total[:,0], y=smart_history_X_total[:,1],mode='markers',\n",
    "                    name = \"smart\",\n",
    "                    marker=dict(size=4,\n",
    "                        color=np.linspace(0,Ns,Ns+1), #set color equal to a variable\n",
    "                        colorscale='blues', # one of plotly colorscales\n",
    "                        showscale=True,\n",
    "                        colorbar=dict(title=\"Smart\")\n",
    "                    )))\n",
    "                fig.add_trace(go.Scatter(x=naive_history_X_total[:,0], y=naive_history_X_total[:,1],mode='markers',\n",
    "                    name = \"naive\",\n",
    "                    marker=dict(size=4,\n",
    "                        color=np.linspace(0,Ns,Ns+1), #set color equal to a variable\n",
    "                        colorscale='reds', # one of plotly colorscales\n",
    "                        showscale=True,\n",
    "                        colorbar=dict(title=\"Naive\",x=1.15)\n",
    "                    )))\n",
    "                fig.update_layout(\n",
    "                    title=\"Trajectory for episode \" + str(ep),\n",
    "                    xaxis_title=\"$x$\",\n",
    "                    yaxis_title=\"$z$\",\n",
    "                    legend_orientation=\"h\",\n",
    "                    showlegend = False\n",
    "                )\n",
    "                fig.show()\n",
    "            else:\n",
    "                plt.figure(figsize=(14,6))\n",
    "                plt.scatter(smart_history_X_total[:,0], smart_history_X_total[:,1], c=np.linspace(0,Ns,Ns+1), \\\n",
    "                            cmap='Blues')\n",
    "                plt.scatter(naive_history_X_total[:,0], naive_history_X_total[:,1], c=np.linspace(0,Ns,Ns+1), \\\n",
    "                            cmap='Reds')\n",
    "                plt.xlabel(\"x\")\n",
    "                plt.ylabel(\"z\")\n",
    "                plt.title(\"Trajectory for episode \" + str(ep))\n",
    "                plt.show()\n",
    "\n",
    "    if i == len(smart_stored_histories) - 1: fig.write_image(\"Figures/\" + directory_name + \"/final-epsiode.pdf\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous\n",
    "### Difference between using explicit euler and RK45 is minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# test_eul = Swimmer()\n",
    "# test_rk45 = copy.deepcopy(test_eul)\n",
    "\n",
    "# for i in range(Ns):\n",
    "#     test_eul.update_kinematics(\"euler\")\n",
    "#     test_rk45.update_kinematics(\"rk45\")\n",
    "\n",
    "# history_eul = np.array(test_eul.history_X_total)\n",
    "# history_rk45 = np.array(test_rk45.history_X_total)\n",
    "\n",
    "# fig = go.Figure(go.Scatter(x=history_eul[:,0], y=history_eul[:,1],mode='markers',\n",
    "#     name = \"smart\",\n",
    "#     marker=dict(size=4,\n",
    "#         color=np.linspace(0,Ns,Ns+1), #set color equal to a variable\n",
    "#         colorscale='blues', # one of plotly colorscales\n",
    "#         showscale=True,\n",
    "#         colorbar=dict(title=\"Euler\")\n",
    "#     )))\n",
    "# fig.add_trace(go.Scatter(x=history_rk45[:,0], y=history_rk45[:,1],mode='markers',\n",
    "#     name = \"naive\",\n",
    "#     marker=dict(size=4,\n",
    "#         color=np.linspace(0,Ns,Ns+1), #set color equal to a variable\n",
    "#         colorscale='reds', # one of plotly colorscales\n",
    "#         showscale=True,\n",
    "#         colorbar=dict(title=\"RK45\",x=1.15)\n",
    "#     )))\n",
    "# fig.update_layout(\n",
    "#     title=\"Explicit euler vs. RK45\",\n",
    "#     xaxis_title=\"$x$\",\n",
    "#     yaxis_title=\"$z$\",\n",
    "#     legend_orientation=\"h\",\n",
    "#     showlegend = False\n",
    "# )\n",
    "# fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of colorscales for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of colorscales\n",
    "# ['aggrnyl', 'agsunset', 'algae', 'amp', 'armyrose', 'balance',\n",
    "#              'blackbody', 'bluered', 'blues', 'blugrn', 'bluyl', 'brbg',\n",
    "#              'brwnyl', 'bugn', 'bupu', 'burg', 'burgyl', 'cividis', 'curl',\n",
    "#              'darkmint', 'deep', 'delta', 'dense', 'earth', 'edge', 'electric',\n",
    "#              'emrld', 'fall', 'geyser', 'gnbu', 'gray', 'greens', 'greys',\n",
    "#              'haline', 'hot', 'hsv', 'ice', 'icefire', 'inferno', 'jet',\n",
    "#              'magenta', 'magma', 'matter', 'mint', 'mrybm', 'mygbm', 'oranges',\n",
    "#              'orrd', 'oryel', 'peach', 'phase', 'picnic', 'pinkyl', 'piyg',\n",
    "#              'plasma', 'plotly3', 'portland', 'prgn', 'pubu', 'pubugn', 'puor',\n",
    "#              'purd', 'purp', 'purples', 'purpor', 'rainbow', 'rdbu', 'rdgy',\n",
    "#              'rdpu', 'rdylbu', 'rdylgn', 'redor', 'reds', 'solar', 'spectral',\n",
    "#              'speed', 'sunset', 'sunsetdark', 'teal', 'tealgrn', 'tealrose',\n",
    "#              'tempo', 'temps', 'thermal', 'tropic', 'turbid', 'twilight',\n",
    "#              'viridis', 'ylgn', 'ylgnbu', 'ylorbr', 'ylorrd']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
